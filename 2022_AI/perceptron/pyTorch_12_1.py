import torch
# from torch import tensor
# from torchvision import datasets

import torch.nn as nn
# network
import torch.optim as optim
# optimazation 경사하강법

# SGD "경사 하강법" 사용 -> Adam 이라는 더 좋은 방법이 있음
# ReLu - sigmoid 발전형 / y = x 함수 / x가 양수면 그대로 표시, 음수면 0으로 표시 (0 ~ x)


lrate = 0.1
INDIM = 26
H1DIM = 10
H2DIM = 10
OUTDIM = 4

PTTN_NUM = 12

# 26 x 12인 'x' tensor 선언
x = torch.tensor([[1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                   0.0, 0.0, 1.0, 0.0, 0.0,
                   0.0, 0.0, 1.0, 0.0, 0.0,
                   0.0, 0.0, 1.0, 0.0, 0.0,
                   0.0, 0.0, 1.0, 0.0, 0.0],  # T-1
                  [1.0, 1.0, 1.0, 1.0, 1.0, 0.0,
                   0.0, 0.0, 1.0, 0.0, 1.0,
                   0.0, 0.0, 1.0, 0.0, 0.0,
                   0.0, 0.0, 1.0, 0.0, 0.0,
                   0.0, 0.0, 1.0, 0.0, 0.0],  # T-2
                  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                   0.0, 0.0, 1.0, 0.0, 0.0,
                   0.0, 0.0, 1.0, 0.0, 0.0,
                   0.0, 0.0, 1.0, 0.0, 0.0,
                   0.0, 0.0, 1.0, 0.0, 0.0],  # T-3
                  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                   1.0, 0.0, 0.0, 0.0, 0.0,
                   1.0, 0.0, 0.0, 0.0, 0.0,
                   1.0, 0.0, 0.0, 0.0, 0.0,
                   1.0, 1.0, 1.0, 1.0, 1.0],  # C-1
                  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                   1.0, 0.0, 0.0, 0.0, 0.0,
                   1.0, 0.0, 0.0, 0.0, 0.0,
                   1.0, 0.0, 0.0, 0.0, 1.0,
                   1.0, 1.0, 1.0, 1.0, 0.0],  # C-2
                  [1.0, 1.0, 1.0, 1.0, 1.0, 0.0,
                   1.0, 0.0, 0.0, 0.0, 1.0,
                   1.0, 0.0, 0.0, 0.0, 0.0,
                   1.0, 0.0, 0.0, 0.0, 1.0,
                   1.0, 1.0, 1.0, 1.0, 0.0],  # C-3
                  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                   1.0, 0.0, 0.0, 0.0, 0.0,
                   1.0, 1.0, 1.0, 1.0, 1.0,
                   1.0, 0.0, 0.0, 0.0, 0.0,
                   1.0, 1.0, 1.0, 1.0, 1.0],  # E-1
                  [1.0, 1.0, 1.0, 1.0, 1.0, 0.0,
                   1.0, 0.0, 0.0, 0.0, 1.0,
                   1.0, 1.0, 1.0, 1.0, 1.0,
                   1.0, 0.0, 0.0, 0.0, 0.0,
                   1.0, 1.0, 1.0, 1.0, 1.0],  # E-2
                  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                   1.0, 0.0, 0.0, 0.0, 0.0,
                   1.0, 1.0, 1.0, 1.0, 1.0,
                   1.0, 0.0, 0.0, 0.0, 0.0,
                   1.0, 1.0, 1.0, 1.0, 1.0],  # E-3
                  [1.0, 1.0, 0.0, 0.0, 0.0, 0.0,
                   1.0, 0.0, 0.0, 0.0, 0.0,
                   1.0, 0.0, 0.0, 0.0, 0.0,
                   1.0, 0.0, 0.0, 0.0, 1.0,
                   1.0, 1.0, 1.0, 1.0, 0.0],  # L-1
                  [1.0, 1.0, 0.0, 0.0, 0.0, 0.0,
                   1.0, 0.0, 0.0, 0.0, 0.0,
                   1.0, 0.0, 0.0, 0.0, 0.0,
                   1.0, 0.0, 0.0, 0.0, 0.0,
                   1.0, 1.0, 1.0, 1.0, 1.0],  # L-2
                  [1.0, 1.0, 0.0, 0.0, 0.0, 0.0,
                   1.0, 0.0, 0.0, 0.0, 0.0,
                   1.0, 0.0, 0.0, 0.0, 0.0,
                   1.0, 0.0, 0.0, 0.0, 0.0,
                   1.0, 1.0, 1.0, 1.0, 0.5],  # L-3
                  ])

# 12 x 4 인 'yt' tensor 선언
yt = torch.tensor([[1.0, 0.0, 0.0, 0.0],
                   [1.0, 0.0, 0.0, 0.0],
                   [1.0, 0.0, 0.0, 0.0],
                   [0.0, 1.0, 0.0, 0.0],
                   [0.0, 1.0, 0.0, 0.0],
                   [0.0, 1.0, 0.0, 0.0],
                   [0.0, 0.0, 1.0, 0.0],
                   [0.0, 0.0, 1.0, 0.0],
                   [0.0, 0.0, 1.0, 0.0],
                   [0.0, 0.0, 0.0, 1.0],
                   [0.0, 0.0, 0.0, 1.0],
                   [0.0, 0.0, 0.0, 1.0]])
print(x.shape)
print(yt.shape)

# 26 x 4 2차원 tensor 생성, w가 얼마나 변하는지 추적하라 [requires_grad=True]
w = torch.randn(INDIM, OUTDIM, requires_grad=True)
b = torch.randn(OUTDIM, requires_grad=True)
z = torch.matmul(x, w) + b

floss = nn.MSELoss()
optimizer = optim.SGD([w, b], lr=0.01)
# SGD "경사 하강법" 사용 -> Adam 이라는 더 좋은 방법이 있음

print(w)
print(b)
print(z)
print(x.shape, w.shape, z.shape)

optimizer.zero_grad()
z = torch.matmul(x, w) + b
print("z :", z)

loss = floss(z, yt)
loss.backword
optimizer.step()

optimizer.zero_grad()
z = torch.matmul(x, w) + b
print("z :", z)

sigmoid = nn.Sigmoid()

for i in range(20000):
    optimizer.zero_grad()
    zt = torch.matmul(x, w) + b
    z = sigmoid(zt)

    loss = floss(z, yt)
    loss.backword()   # 역전파
    optimizer.step()
    if i % 1000 == 0:
        print("epoch: ", i, "z: ", z)
